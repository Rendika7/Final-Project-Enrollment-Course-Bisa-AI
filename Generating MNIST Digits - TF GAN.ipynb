{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1SpS9RmeVWE_R2s4Y47IAfZZTm9sIZUZ_","authorship_tag":"ABX9TyO1IVDAkRSRBo7qwMAHi3Sc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup Library"],"metadata":{"id":"OvKlwuPdryXi"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"CFcvhmKioQRs","executionInfo":{"status":"ok","timestamp":1717201694167,"user_tz":-420,"elapsed":2682,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.autograd.variable import Variable\n","from torchvision import transforms\n","\n","import os"]},{"cell_type":"code","source":["# Tentukan device yang akan digunakan (GPU jika tersedia, jika tidak, gunakan CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"ulmj-PR2r2Wr","executionInfo":{"status":"ok","timestamp":1717201680456,"user_tz":-420,"elapsed":13,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Setup Data"],"metadata":{"id":"OBVny-aHscxZ"}},{"cell_type":"markdown","source":["Datset = https://www.kaggle.com/datasets/hojjatk/mnist-dataset"],"metadata":{"id":"H136VYxwwJws"}},{"cell_type":"code","source":["import os\n","\n","mainPath = \"/content/drive/MyDrive/Colab Notebooks/1. BISA AI - [Studi Independen]/Computer Vision - Open CV/Enroll Generative Adversarial Networks dengan Pytorch\"\n","dataPath = \"/content/drive/MyDrive/Colab Notebooks/7. Dataset Bisa AI/MNIST_Digits\""],"metadata":{"id":"2dcX2-4Ivj9f","executionInfo":{"status":"ok","timestamp":1717201680457,"user_tz":-420,"elapsed":13,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# # Set-up Env Variable for kaggle Config\n","# import os\n","# os.environ['KAGGLE_CONFIG_DIR'] = mainPath\n","\n","# # Tentukan path untuk menyimpan dataset\n","# path_to_save = dataPath\n","\n","# # Buat direktori jika belum ada\n","# os.makedirs(path_to_save, exist_ok=True)\n","\n","# # Unduh dataset menggunakan perintah kaggle\n","# !kaggle datasets download -d hojjatk/mnist-dataset -p '{path_to_save}'"],"metadata":{"id":"AbPKmguDttv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Ekstrak dataset\n","# !unzip '{path_to_save}/mnist-dataset.zip' -d '{path_to_save}'\n","\n","# # Hapus file zip jika diperlukan\n","# os.remove(f'{path_to_save}/mnist-dataset.zip')"],"metadata":{"id":"kYEkkbVwwFL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Load MNIST dataset\n","(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images = train_images / 255.0\n","\n","# Reshape images to (batch_size, height, width, channels)\n","train_images = train_images.reshape(-1, 28, 28, 1)\n","\n","# Create TensorFlow dataset\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n","\n","# Shuffle and batch the dataset\n","batch_size = 128\n","train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"],"metadata":{"id":"-O8jkb-75T-t","executionInfo":{"status":"ok","timestamp":1717200869398,"user_tz":-420,"elapsed":8325,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"96c308b6-822d-4e17-edfd-f74de139a09d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# Define the discriminator network\n","def discriminator_net():\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n","        tf.keras.layers.Dense(1024, activation='relu'),\n","        tf.keras.layers.Dropout(0.3),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dropout(0.3),\n","        tf.keras.layers.Dense(256, activation='relu'),\n","        tf.keras.layers.Dropout(0.3),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","# Define the generator network\n","def generator_net():\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(256, activation='relu', input_shape=(128,)),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dense(1024, activation='relu'),\n","        tf.keras.layers.Dense(28*28, activation='tanh'),\n","        tf.keras.layers.Reshape((28, 28, 1))\n","    ])\n","    return model\n","\n","# Create instances of discriminator and generator\n","discriminator = discriminator_net()\n","generator = generator_net()\n","\n","# Define loss function and optimizers\n","loss = tf.keras.losses.BinaryCrossentropy()\n","d_optimizer = tf.keras.optimizers.Adam(0.0002)\n","g_optimizer = tf.keras.optimizers.Adam(0.0002)"],"metadata":{"id":"D63PRwu05PcT","executionInfo":{"status":"ok","timestamp":1717202580714,"user_tz":-420,"elapsed":719,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Print model summaries\n","discriminator.summary()\n","generator.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlutbP92NyTn","executionInfo":{"status":"ok","timestamp":1717202580715,"user_tz":-420,"elapsed":29,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}},"outputId":"229a99d0-d5ad-4228-f784-57cb337ef24b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_4 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_32 (Dense)            (None, 1024)              803840    \n","                                                                 \n"," dropout_12 (Dropout)        (None, 1024)              0         \n","                                                                 \n"," dense_33 (Dense)            (None, 512)               524800    \n","                                                                 \n"," dropout_13 (Dropout)        (None, 512)               0         \n","                                                                 \n"," dense_34 (Dense)            (None, 256)               131328    \n","                                                                 \n"," dropout_14 (Dropout)        (None, 256)               0         \n","                                                                 \n"," dense_35 (Dense)            (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 1460225 (5.57 MB)\n","Trainable params: 1460225 (5.57 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_36 (Dense)            (None, 256)               33024     \n","                                                                 \n"," dense_37 (Dense)            (None, 512)               131584    \n","                                                                 \n"," dense_38 (Dense)            (None, 1024)              525312    \n","                                                                 \n"," dense_39 (Dense)            (None, 784)               803600    \n","                                                                 \n"," reshape_4 (Reshape)         (None, 28, 28, 1)         0         \n","                                                                 \n","=================================================================\n","Total params: 1493520 (5.70 MB)\n","Trainable params: 1493520 (5.70 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Training loop\n","EPOCHS = 200\n","\n","for epoch in range(EPOCHS):\n","    for batch in train_dataset:\n","        # Flatten the batch of images\n","        real_data = tf.reshape(batch[0], (-1, 28, 28, 1))  # Assuming batch[0] contains the images\n","\n","        # Train Discriminator\n","        with tf.GradientTape() as tape:\n","            # Train on real data\n","            pred_real = discriminator(real_data)\n","            error_real = loss(tf.ones_like(pred_real), pred_real)\n","\n","            # Train on fake data\n","            noise_tensor = tf.random.normal((batch.shape[0], 128))\n","            fake_data = generator(noise_tensor)\n","            pred_fake = discriminator(fake_data)\n","            error_fake = loss(tf.zeros_like(pred_fake), pred_fake)\n","\n","            # Total discriminator loss\n","            d_error = error_real + error_fake\n","\n","        # Compute gradients and update discriminator weights\n","        d_gradients = tape.gradient(d_error, discriminator.trainable_variables)\n","        d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n","\n","        # Train Generator\n","        with tf.GradientTape() as tape:\n","            # Generate fake data\n","            noise_tensor = tf.random.normal((batch.shape[0], 128))\n","            fake_data = generator(noise_tensor)\n","\n","            # Get Discriminator's prediction on fake data\n","            pred = discriminator(fake_data)\n","\n","            # Generator loss\n","            g_error = loss(tf.ones_like(pred), pred)\n","\n","        # Compute gradients and update generator weights\n","        g_gradients = tape.gradient(g_error, generator.trainable_variables)\n","        g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n","\n","    # Log and display progress after each epoch\n","    print(\n","        f\"Epoch [{epoch+1}/{EPOCHS}], D Loss: {d_error.numpy():.4f}, G Loss: {g_error.numpy():.4f}\"\n","    )\n","\n","    # Display generated images after each epoch\n","    test_noise = tf.random.normal((16, 128))\n","    test_images = generator(test_noise)\n","    fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n","    for i, ax in enumerate(axs.flatten()):\n","        ax.imshow(tf.squeeze(test_images[i]), cmap='gray')\n","        ax.axis('off')\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WOM3cmvXnVnC4GvzPdoKYQTPoOqWprKM"},"id":"x3MT3cDN5fLh","outputId":"80721e52-7add-4be3-e6f9-93ca60af939a","executionInfo":{"status":"ok","timestamp":1717209051158,"user_tz":-420,"elapsed":6457375,"user":{"displayName":"RENDIKA NURHARTANTO","userId":"08654841777874618172"}}},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"OJVtSuhA6BYL"},"execution_count":null,"outputs":[]}]}